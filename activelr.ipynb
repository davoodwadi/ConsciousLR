{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "\n",
    "# from ConsciousLR import ConsciousLR\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, sample_size=1000, model='2x', fill = 1 ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.fc1 = nn.Linear(1, 1, bias=False)\n",
    "        self.fc1.weight.data.fill_(fill)\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        return out\n",
    "\n",
    "        \n",
    "\n",
    "torch.manual_seed(1)\n",
    "sample_size = 1000\n",
    "x = torch.randn(sample_size, 1)\n",
    "y = x*2\n",
    "y.shape\n",
    "\n",
    "net1 = Net()\n",
    "net2 = Net()\n",
    "\n",
    "class Crit:\n",
    "    def __init__(self, measure=\"MSE\"):\n",
    "        self.measure = measure\n",
    "    def __call__(self, pred, y):\n",
    "        if self.measure==\"MSE\":\n",
    "            return (pred-y).pow(2).mean()\n",
    "        elif self.measure==\"MAE\":\n",
    "            return (pred-y).abs().mean()\n",
    "        else:\n",
    "            return ((pred-y).pow(4)-8*(pred-y).pow(2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "# import numpy as np\n",
    "class ConsciousLR(Optimizer):\n",
    "    r\"\"\"Implements AdamW algorithm.\n",
    "\n",
    "    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n",
    "    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "            (default: False)\n",
    "\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _Decoupled Weight Decay Regularization:\n",
    "        https://arxiv.org/abs/1711.05101\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, stepSize, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False, lrHigh=.05, lrLow=.95):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad,\n",
    "                        lrHigh=lrHigh, lrLow=lrLow, stepSize=stepSize)\n",
    "        super(ConsciousLR, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(ConsciousLR, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # Perform optimization step\n",
    "                grad = p.grad\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state['gai'] = torch.ones_like(p, memory_format=torch.preserve_format)\n",
    "                    state['cumm'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                # Accumulate gradients for the epoch\n",
    "                state['cumm']+=(p.grad)\n",
    "#                 print('step', state['step'], 'cumm', state['cumm'], 'grad', p.grad.item())\n",
    "\n",
    "\n",
    "                # Perform stepweight decay\n",
    "#                 p.mul_(1 - group['lr'] * state['gai'] * group['weight_decay'])\n",
    "\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "#                 print('exp_avg',state['exp_avg'])\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "#                 exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                state['exp_avg'] = beta1 * (exp_avg) + (1-beta1)*(grad)\n",
    "#                 print('exp_avg',state['exp_avg'])\n",
    "#                 exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                state['exp_avg_sq'] = beta2 * exp_avg_sq + (1-beta2)*grad.pow(2)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "#                 else:\n",
    "#                     denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "\n",
    "                # Correction\n",
    "                exp_avgCorr = state['exp_avg']/(1-beta1**state['step'])\n",
    "#                 print('exp_avgCorr',exp_avgCorr)\n",
    "                exp_avg_sqCorr = state['exp_avg_sq']/(1-beta2**state['step'])\n",
    "\n",
    "#                 step_size = group['lr'] / bias_correction1\n",
    "                step_size = group['lr']\n",
    "#                 gai = state['gai']\n",
    "#                 numer = exp_avg.mul(state['gai'])\n",
    "#                 numer = exp_avg * gai\n",
    "#                 print(numer, ' equals ', exp_avg, gai, state['gai'])\n",
    "#                 p.addcdiv_(numer, denom, value=-step_size)\n",
    "                p -= step_size*state['gai']*(exp_avgCorr/(exp_avg_sqCorr.sqrt()+group['eps']))\n",
    "                # SetLR if i>0\n",
    "                if state['step']/group['stepSize'] > 1 and state['step']%group['stepSize']==0:\n",
    "                    tmp2 = state['gradOld'].clone().cpu()##could be eliminated\n",
    "                    tmp3 = state['cumm'].clone().cpu()##could be eliminated\n",
    "                    tmp5 = state['gai'].clone().cpu()##may be the one that needs cloning\n",
    "#                     print(f'old {tmp2}, cumm {tmp3}')\n",
    "#                     print(state['step'],' is ', state['gai'].item())\n",
    "                    state['gai'] = torch.where(tmp2*tmp3<=0, tmp5.mul(group['lrLow']), tmp5.add(group['lrHigh']))\n",
    "#                     print(state['step'],' is ', state['gai'].item())\n",
    "\n",
    "                # Resetting the accumulated gradients after each epoch\n",
    "                if state['step']%group['stepSize']==0:\n",
    "                    cumm = state['cumm']\n",
    "                    state['gradOld'] = cumm.clone()\n",
    "                    state['cumm'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "# import numpy as np\n",
    "class ConsciousSGD(Optimizer):\n",
    "    r\"\"\"Implements AdamW algorithm.\n",
    "\n",
    "    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n",
    "    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n",
    "\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "            (default: False)\n",
    "\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _Decoupled Weight Decay Regularization:\n",
    "        https://arxiv.org/abs/1711.05101\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, stepSize, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False, lrHigh=.05, lrLow=.95):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad,\n",
    "                        lrHigh=lrHigh, lrLow=lrLow, stepSize=stepSize)\n",
    "        super(ConsciousSGD, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(ConsciousSGD, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # Perform optimization step\n",
    "                grad = p.grad\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                    state['gai'] = torch.ones_like(p, memory_format=torch.preserve_format)\n",
    "                    state['cumm'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "                # Accumulate gradients for the epoch\n",
    "                state['cumm']+=(p.grad)\n",
    "#                 print('step', state['step'], 'cumm', state['cumm'], 'grad', p.grad.item())\n",
    "\n",
    "\n",
    "                # Perform stepweight decay\n",
    "#                 p.mul_(1 - group['lr'] * state['gai'] * group['weight_decay'])\n",
    "\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "#                 print('exp_avg',state['exp_avg'])\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "#                 exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                state['exp_avg'] = beta1 * (exp_avg) + (1-beta1)*(grad)\n",
    "#                 print('exp_avg',state['exp_avg'])\n",
    "#                 exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                state['exp_avg_sq'] = beta2 * exp_avg_sq + (1-beta2)*grad.pow(2)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "#                 else:\n",
    "#                     denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "\n",
    "                # Correction\n",
    "                exp_avgCorr = state['exp_avg']/(1-beta1**state['step'])\n",
    "#                 print('exp_avgCorr',exp_avgCorr)\n",
    "                exp_avg_sqCorr = state['exp_avg_sq']/(1-beta2**state['step'])\n",
    "\n",
    "#                 step_size = group['lr'] / bias_correction1\n",
    "                step_size = group['lr']\n",
    "#                 gai = state['gai']\n",
    "#                 numer = exp_avg.mul(state['gai'])\n",
    "#                 numer = exp_avg * gai\n",
    "#                 print(numer, ' equals ', exp_avg, gai, state['gai'])\n",
    "#                 p.addcdiv_(numer, denom, value=-step_size)\n",
    "                p -= step_size*state['gai']*p.grad\n",
    "                # SetLR if i>0\n",
    "                if state['step']/group['stepSize'] > 1 and state['step']%group['stepSize']==0:\n",
    "                    tmp2 = state['gradOld'].clone().cpu()##could be eliminated\n",
    "                    tmp3 = state['cumm'].clone().cpu()##could be eliminated\n",
    "                    tmp5 = state['gai'].clone().cpu()##may be the one that needs cloning\n",
    "#                     print(f'old {tmp2}, cumm {tmp3}')\n",
    "#                     print(state['step'],' is ', state['gai'].item())\n",
    "                    state['gai'] = torch.where(tmp2*tmp3<=0, tmp5.mul(group['lrLow']), tmp5.add(group['lrHigh']))\n",
    "#                     print(state['step'],' is ', state['gai'].item())\n",
    "\n",
    "                # Resetting the accumulated gradients after each epoch\n",
    "                if state['step']%group['stepSize']==0:\n",
    "                    cumm = state['cumm']\n",
    "                    state['gradOld'] = cumm.clone()\n",
    "                    state['cumm'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = widgets.Output()\n",
    "out0 = widgets.Output()\n",
    "plt.ioff()\n",
    "plt.close('all');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}\n",
    "lossType = widgets.ToggleButtons(\n",
    "    options=['MSE', 'Saddle'],\n",
    "    description='Loss Function',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltips=['Mean Squared Error Loss', 'A Custom Loss Function with a Saddle'],\n",
    "    value='Saddle',\n",
    "    style=style\n",
    "#     icons=['check'] * 3\n",
    ")\n",
    "measure = lossType.value\n",
    "\n",
    "outw = widgets.Output()\n",
    "# layout = widgets.Layout(width='auto')\n",
    "\n",
    "www = widgets.FloatSlider(min=0, max=4, description=\"Initial Weight\", style=style)\n",
    "ep = widgets.IntText(description=\"Number of Epochs\", value=10, style=style)\n",
    "lrs = widgets.FloatSlider(value=-2,min=-3,max=-0.4, description='Learning Rate (log)', style=style)\n",
    "# widgets.VBox([www,ep,lrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_click(b):\n",
    "    global weight_space\n",
    "    global losses\n",
    "    global crit\n",
    "    global measure\n",
    "    global x1data, x2data, y1data, y2data\n",
    "    global startW\n",
    "    global ax1,ax2, fig\n",
    "    net1.fc1.weight.data.fill_(www.get_interact_value()+0.0001); net2.fc1.weight.data.fill_(www.get_interact_value()+0.0001)\n",
    "#     print(lrs.get_interact_value())\n",
    "    lr = 10**lrs.get_interact_value()\n",
    "    opt1 = optim.Adam(net1.parameters(),lr=lr)\n",
    "    if measure=='MSE':\n",
    "        opt2 = ConsciousSGD(net2.parameters(), 1, lr=lr, lrLow=.1, lrHigh=2.)\n",
    "    else:\n",
    "        opt2 = ConsciousLR(net2.parameters(), 1, lr=lr, lrLow=.1, lrHigh=2.)\n",
    "    for _ in range(ep.value):\n",
    "#         ax1.clear()\n",
    "#         ax2.clear()\n",
    "        time.sleep(1)\n",
    "        w_a = net1.fc1.weight.data.item()\n",
    "        w_c = net2.fc1.weight.data.item()\n",
    "\n",
    "    #     time.sleep(1.)\n",
    "        opt1.zero_grad()\n",
    "        l1 = crit(net1(x), y)\n",
    "        l1.backward()\n",
    "        opt1.step()\n",
    "\n",
    "        opt2.zero_grad()\n",
    "        l2 = crit(net2(x), y)\n",
    "        l2.backward()\n",
    "        opt2.step()\n",
    "\n",
    "\n",
    "\n",
    "        x1data.append(w_a)\n",
    "        x2data.append(w_c)\n",
    "\n",
    "        y1data.append(l1.item())\n",
    "        y2data.append(l2.item())\n",
    "        \n",
    "        ##############\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2, figsize=(7,4), sharey=True)\n",
    "        ax1.plot(weight_space ,losses[0], c='red')\n",
    "        ax2.plot(weight_space ,losses[0], c='red')\n",
    "        ax1.set_ylabel('loss')\n",
    "        fig.text(0.5, 0.04, 'weight', ha='center')\n",
    "        ax1.set_title('Adam')\n",
    "        ax2.set_title(\"ActiveLR\")\n",
    "        ax1.set_xlim(torch.min(weight_space).item(), torch.max(weight_space).item())\n",
    "        ax1.set_ylim((losses[0]).min(), (losses[0]).max())\n",
    "\n",
    "        ax2.set_xlim(torch.min(weight_space).item(), torch.max(weight_space).item())\n",
    "        ax2.set_ylim((losses[0]).min(), (losses[0]).max())\n",
    "\n",
    "\n",
    "        ##############\n",
    "        \n",
    "        ax1.plot(x1data, y1data, 'blue', lw=3, alpha=0.4)\n",
    "        ax1.scatter(x1data[-1], y1data[-1], color='green', marker='v', s=40)\n",
    "        ax2.plot(x2data, y2data, 'black', lw=3, alpha=0.4)\n",
    "        ax2.scatter(x2data[-1], y2data[-1], color='green', marker='v', s=40)\n",
    "        with out2:\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(fig)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossHandler(change):\n",
    "    global weight_space\n",
    "    global losses\n",
    "    global crit\n",
    "    global measure\n",
    "    global x1data, x2data, y1data, y2data\n",
    "    global startW\n",
    "    global fig, ax1, ax2\n",
    "#     if change['new']\n",
    "    out0.clear_output(wait=True)\n",
    "    out2.clear_output(wait=True)\n",
    "#     time.sleep(1)\n",
    "\n",
    "    \n",
    "    measure = change['new']\n",
    "    \n",
    "#     fig, ax = plt.subplots(1,2)\n",
    "#     for a, loss in zip(ax, losses):\n",
    "#         a.plot(weight_space ,loss, c='red')\n",
    "            \n",
    "    steps = 50\n",
    "    weight_space = torch.linspace(0, 4, steps=steps)\n",
    "    crit = Crit(measure=measure)\n",
    "\n",
    "    loss1 = np.zeros((steps))\n",
    "    loss2 = np.zeros((steps))\n",
    "    for i, w in enumerate(weight_space):\n",
    "        net1.fc1.weight.data.fill_(w)\n",
    "        net2.fc1.weight.data.fill_(w)\n",
    "        with torch.no_grad():\n",
    "            loss1[i] = crit(net1(x), y)\n",
    "            loss2[i] = crit(net2(x), y)\n",
    "    losses = [loss1, loss2]\n",
    "#     print(lrs.get_interact_value())\n",
    "    lr = 10**lrs.get_interact_value()\n",
    "\n",
    "    out2.clear_output(wait=True)\n",
    "    opt1 = optim.Adam(net1.parameters(),lr=lr)\n",
    "    opt2 = ConsciousLR(net2.parameters(), 1, lr=lr, lrLow=.1, lrHigh=2)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(7,4), sharey=True)\n",
    "    ax1.plot(weight_space ,losses[0], c='red')\n",
    "    ax2.plot(weight_space ,losses[0], c='red')\n",
    "    ax1.set_ylabel('loss')\n",
    "    fig.text(0.5, 0.04, 'weight', ha='center')\n",
    "    ax1.set_title('Adam')\n",
    "    ax2.set_title(\"ActiveLR\")\n",
    "    \n",
    "    # ax1, ax2 = ax\n",
    "    x1data, x2data, y1data, y2data = [], [], [], []\n",
    "    # ln1, = ax1.plot([], [], 'blue', lw=3, alpha=0.4)\n",
    "    # ln2, = ax2.plot([], [], 'black', lw=3, alpha=0.4)\n",
    "    # line = [ln1, ln2]\n",
    "\n",
    "    ax1.set_xlim(torch.min(weight_space).item(), torch.max(weight_space).item())\n",
    "    ax1.set_ylim((losses[0]).min(), (losses[0]).max())\n",
    "\n",
    "    ax2.set_xlim(torch.min(weight_space).item(), torch.max(weight_space).item())\n",
    "    ax2.set_ylim((losses[0]).min(), (losses[0]).max())\n",
    "\n",
    "\n",
    "    button = widgets.Button(description='Train')\n",
    "    button.on_click(train_click)\n",
    "\n",
    "\n",
    "    out2.clear_output(wait=True)\n",
    "    with out0:\n",
    "        display.display(lossType, button,)\n",
    "    with out2:\n",
    "        display.display(fig)\n",
    "\n",
    "lossType.observe(lossHandler, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# measure='MSE'\n",
    "steps = 50\n",
    "weight_space = torch.linspace(0, 4, steps=steps)\n",
    "crit = Crit(measure=measure)\n",
    "\n",
    "pause=False\n",
    "\n",
    "loss1 = np.zeros((steps))\n",
    "loss2 = np.zeros((steps))\n",
    "for i, w in enumerate(weight_space):\n",
    "    net1.fc1.weight.data.fill_(w)\n",
    "    net2.fc1.weight.data.fill_(w)\n",
    "    with torch.no_grad():\n",
    "        loss1[i] = crit(net1(x), y)\n",
    "        loss2[i] = crit(net2(x), y)\n",
    "losses = [loss1, loss2]\n",
    "# print(lrs.get_interact_value())\n",
    "lr = 10**lrs.get_interact_value()\n",
    "\n",
    "out2.clear_output(wait=True)\n",
    "opt1 = optim.Adam(net1.parameters(),lr=lr)\n",
    "if measure=='MSE':\n",
    "    opt2 = ConsciousSGD(net2.parameters(), 1, lr=lr, lrLow=.1, lrHigh=2.)\n",
    "else:\n",
    "    opt2 = ConsciousLR(net2.parameters(), 1, lr=lr, lrLow=.1, lrHigh=2.)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(7,4), sharey=True)\n",
    "ax1.plot(weight_space ,losses[0], c='red');\n",
    "ax2.plot(weight_space ,losses[0], c='red');\n",
    "ax1.set_ylabel('loss')\n",
    "fig.text(0.5, 0.04, 'weight', ha='center')\n",
    "ax1.set_title('Adam')\n",
    "ax2.set_title(\"ActiveLR\")\n",
    "\n",
    "x1data, x2data, y1data, y2data = [], [], [], []\n",
    "\n",
    "ax1.set_xlim(torch.min(weight_space).item(), torch.max(weight_space).item())\n",
    "ax1.set_ylim((losses[0]).min(), (losses[0]).max())\n",
    "\n",
    "ax2.set_xlim(torch.min(weight_space).item(), torch.max(weight_space).item())\n",
    "ax2.set_ylim((losses[0]).min(), (losses[0]).max())\n",
    "\n",
    "\n",
    "   \n",
    "button = widgets.Button(description='Train')\n",
    "button.on_click(train_click)\n",
    "\n",
    "\n",
    "out2.clear_output(wait=True)\n",
    "with out0:\n",
    "    display.display(lossType, button)\n",
    "with out2:\n",
    "    display.display(fig)\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightHandler(change):\n",
    "    global startW\n",
    "    startW=change['new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "www.observe(weightHandler, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "outw.clear_output(wait=True)\n",
    "with outw:\n",
    "    display.display(www, ep, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39af823402b94f249571d8b8207447b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': \"FloatSlider(value=0.0, description='In…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753dbd549cf749bd87ccbf24ee1ed7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': \"ToggleButtons(description='Loss Functi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85cbac556cd4b9d8cd0abe7576848de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 504x288 with 2 Axes>', 'i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
